import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from models import AutoEncoder

# ==== å­—ä½“è®¾ç½®ï¼ˆé¿å…ä¸­æ–‡+ä¸‹æ ‡è­¦å‘Šï¼‰ ====
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'Arial']
plt.rcParams['axes.unicode_minus'] = False

# ==== è¯»å–æ‰€æœ‰ SpO2 æ•°æ® ====
def load_all_spo2_data(data_dir="data"):
    all_sequences = []
    file_map = {}  # æ˜ å°„æ¯æ®µå±äºå“ªä¸ªæ–‡ä»¶
    for filename in os.listdir(data_dir):
        if filename.endswith(".txt"):
            path = os.path.join(data_dir, filename)
            values = []
            try:
                with open(path, 'r') as f:
                    for line in f:
                        try:
                            values.append(float(line.strip()))
                        except ValueError:
                            continue
                if len(values) >= 60:
                    all_sequences.append(values)
                    file_map[filename] = values
                    print(f"âœ… è¯»å– {filename}ï¼ˆ{len(values)} æ¡ï¼‰")
            except Exception as e:
                print(f"âŒ è¯»å–å¤±è´¥ {filename}: {e}")
    return all_sequences, file_map

# ==== åˆ‡ç‰‡å‡½æ•° ====
def slice_sequences(sequences, window_size=60):
    segments = []
    segment_origin = []
    for idx, seq in enumerate(sequences):
        t = torch.tensor(seq, dtype=torch.float32)
        for i in range(len(t) - window_size):
            segments.append(t[i:i + window_size])
            segment_origin.append((idx, i))  # ç¬¬ idx ä¸ªåºåˆ—çš„ç¬¬ i ä¸ªçª—å£
    return torch.stack(segments), segment_origin

# ==== åŠ è½½æ•°æ® ====
all_sequences, file_map = load_all_spo2_data("data")
segments_tensor, segment_origin = slice_sequences(all_sequences, window_size=60)
print("ğŸ“Š æ€»æ ·æœ¬æ•°ï¼š", len(segments_tensor))

# ==== åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨ ====
dataset = TensorDataset(segments_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# ==== åˆå§‹åŒ–æ¨¡å‹ ====
model = AutoEncoder(input_dim=60)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ==== è®­ç»ƒæ¨¡å‹ ====
epochs = 100
for epoch in range(epochs):
    total_loss = 0
    for (batch,) in dataloader:
        output = model(batch)
        loss = criterion(output, batch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"ğŸ“˜ Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(dataloader):.6f}")

# ==== å¼‚å¸¸æ£€æµ‹ ====
model.eval()
reconstruction_errors = []
with torch.no_grad():
    for (batch,) in dataloader:
        output = model(batch)
        loss = torch.mean((output - batch) ** 2, dim=1)
        reconstruction_errors.extend(loss.numpy())

reconstruction_errors = np.array(reconstruction_errors)
threshold = np.percentile(reconstruction_errors, 95)
anomaly_indices = np.where(reconstruction_errors > threshold)[0]
print(f"ğŸš¨ å¼‚å¸¸é˜ˆå€¼: {threshold}")
print(f"â— å¼‚å¸¸ç‰‡æ®µæ•°é‡: {len(anomaly_indices)} / {len(reconstruction_errors)}")

# ==== ä¿å­˜é‡æ„è¯¯å·®å›¾ ====
plt.figure(figsize=(10, 4))
plt.plot(reconstruction_errors, label="é‡æ„è¯¯å·®")
plt.axhline(threshold, color='r', linestyle='--', label="é˜ˆå€¼")
plt.scatter(anomaly_indices, reconstruction_errors[anomaly_indices], color='red', s=10, label="å¼‚å¸¸")
plt.title("SpO2 å¼‚å¸¸æ£€æµ‹ - é‡æ„è¯¯å·®åˆ†å¸ƒ")
plt.xlabel("æ ·æœ¬åºå·")
plt.ylabel("è¯¯å·®")
plt.legend()
plt.tight_layout()
os.makedirs("output", exist_ok=True)
plt.savefig("output/anomaly_detection.png")
print("âœ… é‡æ„è¯¯å·®å›¾å·²ä¿å­˜ï¼šoutput/anomaly_detection.png")

# ==== å¯è§†åŒ–ï¼šæ ‡æ³¨åŸå§‹æ³¢å½¢ä¸­çš„å¼‚å¸¸ç‰‡æ®µï¼ˆä»¥ç¬¬ä¸€ä¸ªæ–‡ä»¶ä¸ºä¾‹ï¼‰ ====
# æ‰¾å‡ºç¬¬ä¸€ä¸ªæ–‡ä»¶å
first_file = list(file_map.keys())[0]
original_data = file_map[first_file]
original_tensor = torch.tensor(original_data)

# æ‰¾å‡ºè¯¥æ–‡ä»¶ä¸­å“ªäº›ç‰‡æ®µè¢«åˆ¤å®šä¸ºå¼‚å¸¸
file_indices = [i for i, (f_idx, start) in enumerate(segment_origin) if f_idx == 0]
file_anomalies = [i for i in file_indices if i in anomaly_indices]
highlight_ranges = [segment_origin[i][1] for i in file_anomalies]

# ç»˜åˆ¶åŸå§‹æ³¢å½¢ + å¼‚å¸¸åŒºåŸŸ
plt.figure(figsize=(12, 4))
plt.plot(original_tensor, label="SpO2 åŸå§‹æ³¢å½¢")

for start in highlight_ranges:
    plt.axvspan(start, start + 60, color='red', alpha=0.3)

plt.title(f"{first_file} è¡€æ°§æ³¢å½¢ä¸­å¼‚å¸¸ç‰‡æ®µæ ‡æ³¨")
plt.xlabel("æ—¶é—´ï¼ˆç§’ï¼‰")
plt.ylabel("SpO2 (%)")
plt.legend()
plt.tight_layout()
plt.savefig("output/spo2_with_anomalies.png")
print("âœ… æ ‡æ³¨å›¾å·²ä¿å­˜ï¼šoutput/spo2_with_anomalies.png")

import pandas as pd

# === åˆ›å»ºä¿å­˜å›¾åƒç›®å½• ===
plot_dir = os.path.join("output", "anomaly_plots")
os.makedirs(plot_dir, exist_ok=True)

stats = []

# éå†æ¯ä¸ªæ–‡ä»¶
for file_idx, filename in enumerate(file_map.keys()):
    original_data = file_map[filename]
    original_tensor = torch.tensor(original_data)

    # æ‰¾å‡ºè¯¥æ–‡ä»¶æ‰€æœ‰ç‰‡æ®µåœ¨ segment_origin ä¸­çš„ä½ç½®
    file_segment_ids = [i for i, (fidx, _) in enumerate(segment_origin) if fidx == file_idx]
    file_anomaly_ids = [i for i in file_segment_ids if i in anomaly_indices]

    # æ ‡æ³¨è¿™äº›ç‰‡æ®µåœ¨åŸå§‹ä¿¡å·ä¸­çš„ä½ç½®
    highlight_ranges = [segment_origin[i][1] for i in file_anomaly_ids]

    # === ç»˜å›¾ ===
    plt.figure(figsize=(12, 4))
    plt.plot(original_tensor, label="SpO2 åŸå§‹æ³¢å½¢")

    for start in highlight_ranges:
        plt.axvspan(start, start + 60, color='red', alpha=0.3)

    plt.title(f"{filename} è¡€æ°§å¼‚å¸¸ç‰‡æ®µæ ‡æ³¨")
    plt.xlabel("æ—¶é—´ï¼ˆç§’ï¼‰")
    plt.ylabel("SpO2 (%)")
    plt.legend()
    plt.tight_layout()

    # ä¿å­˜å›¾åƒ
    save_path = os.path.join(plot_dir, f"{filename.replace('.txt', '')}.png")
    plt.savefig(save_path)
    plt.close()
    print(f"âœ… å·²ä¿å­˜å›¾ï¼š{save_path}")

    # ç»Ÿè®¡ä¿¡æ¯
    stats.append({
        "æ–‡ä»¶å": filename,
        "æ€»ç‰‡æ®µæ•°": len(file_segment_ids),
        "å¼‚å¸¸ç‰‡æ®µæ•°": len(file_anomaly_ids),
        "å¼‚å¸¸ç‡(%)": round(100 * len(file_anomaly_ids) / len(file_segment_ids), 2)
    })

# === ä¿å­˜å¼‚å¸¸ç»Ÿè®¡è¡¨æ ¼ ===
df = pd.DataFrame(stats)
df.to_csv("output/anomaly_stats.csv", index=False, encoding="utf-8-sig")
print("ğŸ“Š å¼‚å¸¸ç»Ÿè®¡è¡¨å·²ä¿å­˜ä¸ºï¼šoutput/anomaly_stats.csv")

# ==== å¯¼å‡ºä¸º ONNX æ¨¡å‹ ====
onnx_path = "output/autoencoder_spo2.onnx"
dummy_input = torch.randn(1, 60)  # è¾“å…¥å½¢çŠ¶å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´

torch.onnx.export(
    model,                        # è®­ç»ƒå¥½çš„ PyTorch æ¨¡å‹
    dummy_input,                  # æ¨¡æ‹Ÿè¾“å…¥
    onnx_path,                    # è¾“å‡ºè·¯å¾„
    input_names=["input"],
    output_names=["output"],
    dynamic_axes={
        "input": {0: "batch_size"},
        "output": {0: "batch_size"}
    },
    opset_version=11,
    verbose=False
)

print(f"âœ… ONNX æ¨¡å‹å·²å¯¼å‡ºï¼š{onnx_path}")

torch.save(model.state_dict(), "output/spo2_autoencoder.pth")

